---
title: Gradient Descent
description: Gradient Descent for Optimization
date: 2025-10-31
weight: 5
math: true
---

{{% pageinfo %}}
In this section we will understand Gradient Descent for solving optimization problems in Machine Learning.<br>
{{% /pageinfo %}}

{{< playlist "https://www.youtube.com/playlist?list=PLnpa6KP2ZQxfzDh2D3OqVo-piGAZQPWDj" 
        "Calculus for AI & ML | Full Course Videos">}}

{{< alert color="secondary" title="Gradient Based Optimization" >}}
_Till now, we have understood how to formulate a minimization problem as a mathematical optimization problem. <br> 
Now, lets take a step forward and understand how to solve these optimization problems._ <br>

We will focus on two important iterative gradient based methods: <br>
1. **Gradient Descent**: First order method, uses only the gradient.
2. **Newton's Method**: Second order method, used both the gradient and the Hessian.


{{< /alert >}}

<br><br>
```End of Section```