---
title: Log Loss
description: Log Loss
date: 2026-02-13
weight: 222
math: true
---

{{< panel color="blue" title="Visual" >}}
{{< imgproc "images/machine_learning/supervised/logistic_regression/log_loss/slide_01_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="orange" title="Binary Classification" >}}
- Binary Classification
{{< imgproc "images/machine_learning/supervised/logistic_regression/log_loss/slide_02_01.tif" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="green" title="Log Loss" >}}
- Log Loss =
{{< /panel >}}

{{< panel color="red" title="Visual" >}}
{{< imgproc "images/machine_learning/supervised/logistic_regression/log_loss/slide_04_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="navy" title="Cost Function" >}}
- We need to find the weights ğŸ‹ï¸â€â™€ï¸ â€˜w' that minimize the cost ğŸ’µ function.
\[J(w)=-\frac{1}{n}\sum[y_{i}log(y_{i}^{Ì‚})+(1-y_{i})log(1-y_{i}^{Ì‚})]\]
{{< /panel >}}

{{< panel color="blue" title="Gradient Descent" >}}
- Gradient Descent
\[w_{new}=w_{old}-Î·.\frac{âˆ‚J(w)}{âˆ‚w_{old}}\]
{{< /panel >}}

{{< panel color="orange" title="Gradient Calculation" >}}
- Chain Rule:
- Cost Function:
- Prediction:
- Distance of Point:
\[\frac{âˆ‚J(w)}{âˆ‚w}=\frac{âˆ‚J(w)}{âˆ‚y^{Ì‚}}.\frac{âˆ‚y^{Ì‚}}{âˆ‚z}.\frac{âˆ‚z}{âˆ‚w}\]
{{< /panel >}}

{{< panel color="green" title="Cost ğŸ’°Function Derivative" >}}
- How loss changes w.r.t prediction ?
\[J(w)=-\sum[ylog(y^{Ì‚})+(1-y)log(1-y^{Ì‚})]\]
\[\frac{âˆ‚J(w)}{âˆ‚y^{Ì‚}}=-[\frac{y}{y^{Ì‚}}-\frac{1-y}{1-y^{Ì‚}}] \\ =-[\frac{y-yy^{Ì‚}-y^{Ì‚}+yy^{Ì‚}}{y^{Ì‚}(1-y^{Ì‚})}] \\ âˆ´\frac{âˆ‚J(w)}{âˆ‚y^{Ì‚}}=\frac{y^{Ì‚}-y}{y^{Ì‚}(1-y^{Ì‚})}\]
{{< /panel >}}

{{< panel color="red" title="Prediction Derivative" >}}
- How prediction changes w.r.t distance ?
\[y^{Ì‚}=\sigma(z)=\frac{1}{1+e^{-z}}\]
\[\frac{âˆ‚y^{Ì‚}}{âˆ‚z}=\frac{âˆ‚\sigma(z)}{âˆ‚z}=\sigma^{â€²}(z) \\ \sigma^{â€²}(z)=\sigma(z)(1-\sigma(z)) \\ âˆ´\frac{âˆ‚y^{Ì‚}}{âˆ‚z}=y^{Ì‚}(1-y^{Ì‚})\]
{{< /panel >}}

{{< panel color="navy" title="Sigmoid Derivative" >}}
- Sigmoid Derivative
\[\sigma(z)=\frac{1}{1+e^{-z}}\]
\[Letu=1+e^{-z} \\ âŸ¹\sigma(z)=\frac{1}{u},so, \\ \frac{âˆ‚\sigma(z)}{âˆ‚z}=\frac{âˆ‚\sigma(z)}{âˆ‚u}.\frac{âˆ‚u}{âˆ‚z} \\ \frac{âˆ‚\sigma(z)}{âˆ‚u}=-\frac{1}{u^{2}}=-\frac{1}{(1+e^{-z})^{2}} \\ and\frac{âˆ‚(1+e^{-z})}{âˆ‚z}=-e^{-z}\]
{{< /panel >}}

{{< panel color="blue" title="Sigmoid Derivative (continuedâ€¦)" >}}
- Sigmoid Derivative (continuedâ€¦)
\[\sigma(z)=\frac{1}{1+e^{-z}}\]
\[\frac{âˆ‚\sigma(z)}{âˆ‚z}=\frac{âˆ‚\sigma(z)}{âˆ‚u}.\frac{âˆ‚u}{âˆ‚z} \\ \frac{âˆ‚\sigma(z)}{âˆ‚z}=-\frac{1}{(1+e^{-z})^{2}}.-e^{-z}=\frac{e^{-z}}{(1+e^{-z})^{2}} \\ 1-\sigma(z)=1-\frac{1}{1+e^{-z}}=\frac{e^{-z}}{1+e^{-z}} \\ \frac{âˆ‚\sigma(z)}{âˆ‚z}=\frac{1}{1+e^{-z}}.\frac{e^{-z}}{1+e^{-z}} \\ âˆ´\frac{âˆ‚\sigma(z)}{âˆ‚z}=\sigma(z).(1-\sigma(z))\]
{{< /panel >}}

{{< panel color="orange" title="Distance Derivative" >}}
- How distance changes w.r.t weight ğŸ‹ï¸â€â™€ï¸ ?
- ,
\[z=w^{T}x+w_{0}\]
{{< /panel >}}

{{< panel color="green" title="Gradient Calculation (combined)" >}}
- Chain Rule:
{{< /panel >}}

{{< panel color="red" title="Cost ğŸ’°Function Derivative" >}}
- Gradient = Error x Input
- Error = : how far is prediction from the truth?
- Input = : contribution of specific feature to the error.
\[\frac{âˆ‚J(w)}{âˆ‚w}=\sum(y_{i}^{Ì‚}-y_{i}).x_{i}\]
{{< /panel >}}

{{< panel color="navy" title="Gradient Descent (update)" >}}
- Gradient Descent (update)
\[w_{new}=w_{old}-Î·.\sum_{i=1}^{n}(y_{i}^{Ì‚}-y_{i}).x_{i}\]
{{< /panel >}}

{{< panel color="blue" title="Why MSE can NOT be used as Loss Function?" >}}
- Mean Squared Error (MSE) can not be used to quantify error/loss in binary classification because:
- Convexity : MSE combined with Sigmoid is non-convex, so, Gradient Descent can get trapped in local minima.
- Penalty: MSE does not appropriately penalize mis-classifications in binary classification.
{{< /panel >}}

{{< panel color="orange" title="Visual" >}}
{{< imgproc "images/machine_learning/supervised/logistic_regression/log_loss/slide_17_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="green" title="Visual" >}}
{{< imgproc "images/machine_learning/supervised/logistic_regression/log_loss/slide_18_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< video "https://youtu.be/zCBnS3XT61Y" >}}
<br><br>
```End of Section```