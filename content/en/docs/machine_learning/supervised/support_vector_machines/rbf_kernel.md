---
title: RBF Kernel
description: RBF Kernel
date: 2026-02-13
weight: 256
math: true
---

{{< playlist "https://www.youtube.com/playlist?list=PLnpa6KP2ZQxfFECdHEHPM6NjErmLmg2Ff" 
"Support Vector Machine | All Videos" >}}

<br>

{{< panel color="blue" title="Intuition ğŸ’¡" >}}
- ğŸªUnlike the polynomial kernel, which looks at global ğŸŒ interactions, the RBF kernel acts like a similarity measure.
- ğŸ§© If and are identical .
- As they move further apart in Euclidean space, the value decays exponentially towards 0.
{{< /panel >}}

{{< panel color="orange" title="Radial Basis Function (RBF) Kernel" >}}
- where,
- If (very close),
- If , are far apart, Note: Kernel is the measure of similarity or closeness.
\[K(x,z)=exp-\gamma.âˆ¥x-zâˆ¥^{2}\]
{{< /panel >}}

{{< panel color="green" title="Visual" >}}
{{< imgproc "images/machine_learning/supervised/support_vector_machines/rbf_kernel/slide_03_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="red" title="Infinite Dimension Mapping" >}}
- Say, , then Euclidean distance: =
- The Taylor expansion for
\[K(x,z)=exp(-(âˆ¥xâˆ¥^{2}+âˆ¥zâˆ¥^{2}-2x^{T}z))=exp(-âˆ¥xâˆ¥^{2})exp(-âˆ¥zâˆ¥^{2})exp(2x^{T}z)\]
\[exp(2x^{T}z)=\sum_{n=0}^{\infty}\frac{(2x^{T}z)^{n}}{n!}=1+\frac{2x^{T}z}{1!}+\frac{(2x^{T}z)^{2}}{2!}+â€¦+\frac{(2x^{T}z)^{n}}{n!}+â€¦\]
\[K(x,z)=e^{-âˆ¥xâˆ¥^{2}}e^{-âˆ¥zâˆ¥^{2}}n=0\infty\frac{2^{n}(x^{T}z)^{n}}{n!}\]
{{< /panel >}}

{{< panel color="navy" title="Infinite Dimension Mapping (Continued)" >}}
- ğŸª‚If we expand each term, it represents the dot product of all possible norder polynomial features.â›„ï¸Thus, the implicit feature map is:
- Important: The tensor product creates a vector (or matrix) containing all combinations of the features.
- e.g. if , then Note: Because the Taylor series has an infinite number of terms, feature map has an infinite number of dimensions.
\[Ï•(x)=e^{-âˆ¥xâˆ¥^{2}}1,\sqrt{\frac{2}{1!}}x,\sqrt{\frac{2^{2}}{2!}}(xâŠ—x),â€¦,\sqrt{\frac{2^{n}}{n!}}(xâŠ—â€¦âŠ—x_{âŸ}_{ntimes}),â€¦^{T}\]
{{< /panel >}}

{{< panel color="blue" title="Bias-Variance Trade-Off âš”ï¸" >}}
- High Gamma(low ): Over-FittingMakes the kernel so â€˜peaky' that each support vector only influences its immediate neighborhood. Decision boundary becomes highly irregular, â€˜wrapping' tightly around individual data points to ensure they are classified correctly.
- Low Gamma(high ): Under-FittingThe Gaussian bumps are wide and flat. Decision boundary becomes very smooth, essentially behaving more like a linear or low-degree polynomial classifier.
{{< /panel >}}


{{< video "https://youtu.be/TMcJxXUQxzM" >}}
<br><br>
```End of Section```