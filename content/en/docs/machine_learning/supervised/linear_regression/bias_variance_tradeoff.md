---
title: Bias Variance Tradeoff
description: Bias Variance Tradeoff
date: 2026-02-13
weight: 10
math: true
---

{{< playlist "https://www.youtube.com/playlist?list=PLnpa6KP2ZQxde_X17sF7iN6rnQgFlBDC-" 
"Linear Regression | All Videos" >}}
<br>

{{< panel color="blue" title="Total Error" >}}
Mean Squared Error (MSE) = \(\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y_i})^2\)

**Total Error = Bias^2 + Variance + Irreducible Error**

- **Bias** = Systematic Error
- **Variance** = Sensitivity to Data
- **Irreducible Error** = Sensor noise, Human randomness
{{< /panel >}}

{{< panel color="green" title="Bias" >}}
Systematic error from overly simplistic assumptions or strong opinion in the model.

e.g. House ğŸ  prices ğŸ’° = Rs. 10,000 * Area (sq. ft).

**Note**: This is over simplified view, because it ignores, amenities, location, age, etc.
{{< /panel >}}

{{< panel color="orange" title="Variance" >}}
Error from sensitivity to small fluctuations ğŸ“ˆ in the data.

e.g. Deep neural ğŸ§  network trained on a small dataset.

**Note**: Memorizes everything, including noise. <br>

Say a house ğŸ  in XYZ street was sold for very low price ğŸ’°.

**Reason**: Distress selling (outlier), or incorrect entry (noise).

**Note**: Model will make wrong(lower) price ğŸ’°predictions for all houses in XYZ street.
{{< /panel >}}

{{< panel color="charcoal" title="Linear (High Bias), Polynomial(High Variance)" >}}
{{< imgproc "images/machine_learning/supervised/linear_regression/bias_variance_tradeoff/slide_06_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="charcoal" title="Bias Variance Table" >}}
{{< imgproc "images/machine_learning/supervised/linear_regression/bias_variance_tradeoff/bias_variance_table.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="green" title="Bias-Variance Trade-Off" >}}
Goal ğŸ¯ is to minimize total error.

Find a sweet-spot balance âš–ï¸ between Bias and Variance.

A good model **â€˜generalizes'** well, i.e.,
- Is not too simple or has a strong opinion.
- Does not memorize ğŸ§  everything in the data, including noise.
{{< /panel >}}

{{< panel color="blue" title="Fix ğŸ©¹ High Bias (Under-Fitting)" >}}
- Make model more complex.
  - Add more features, add polynomial features.
- Decrease Regularization.
- Train ğŸƒâ€â™‚ï¸longer, the model has not yet converged.
{{< /panel >}}

{{< panel color="navy" title="Fix ğŸ©¹ High Variance (Over-Fitting)" >}}
- Add more data (most effective).
  - Harder to memorize ğŸ§  1 million examples than 100.
  - Use data augmentation, if getting more data is difficult.
- Increase Regularization.
- Early stopping ğŸ›‘, prevents memorization ğŸ§ .
- Dropout (DL), randomly kill neurons, prevents co-adaptation.
- Use Ensembles.
- Averaging reduces variance.

**Note**: *Co-adaptation* refers to a phenomenon where neurons in a neural network become highly dependent on each other to detect features, rather than learning independently.
{{< /panel >}}

{{< video "https://youtu.be/nZtRf_zqY1Y" >}}
<br><br>
```End of Section```