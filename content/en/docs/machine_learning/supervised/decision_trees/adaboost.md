---
title: AdaBoost
description: AdaBoost
date: 2026-02-13
weight: 249
math: true
---

{{< panel color="blue" title="Adaptive Boosting (AdaBoost)" >}}
- Works by increasing ğŸ“ˆ the weight ğŸ‹ï¸â€â™€ï¸ of misclassified data points after each iteration, forcing the next weak learner to
- â€˜pay more attention'ğŸš¨ to the difficult cases.
- â­ï¸ Commonly used for classification.
{{< /panel >}}

{{< panel color="orange" title="Visual" >}}
{{< imgproc "images/machine_learning/supervised/decision_trees/adaboost/slide_02_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="green" title="Decision Stumps" >}}
- Weak learners are typically
- â€˜Decision Stumps', i.e,
- decision treesğŸŒ²with a depth of only one (1 split, 2 leaves ğŸƒ).
{{< /panel >}}

{{< panel color="red" title="Visual" >}}
{{< imgproc "images/machine_learning/supervised/decision_trees/adaboost/slide_04_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="navy" title="Algorithm" >}}
- Assign an equal weight ğŸ‹ï¸â€â™€ï¸to every data point; , where 'n'=number of samples.
- Build a decision stump that minimizes the weighted classification error.
- Calculate total error; .
{{< /panel >}}

{{< panel color="blue" title="Algorithm (Continued)" >}}
- Determine â€˜amount of say', i.e, the weight ğŸ‹ï¸â€â™€ï¸ of each stump in final decision.
- Low error results in a high positive (high influence).
- 50% error (random guessing) results in an = 0 (no influence)
- Update sample weights ğŸ‹ï¸â€â™€ï¸.
- Misclassified samples: Weight ğŸ‹ï¸â€â™€ï¸ increases by .
- Correctly classified samples: Weight ğŸ‹ï¸â€â™€ï¸ decreases by .
- Normalization: All new weights ğŸ‹ï¸â€â™€ï¸ are divided by their total sum so they add up back to 1.
- Iterate for a specified number of estimators (n_estimators).
{{< /panel >}}

{{< panel color="orange" title="Final Prediction ğŸ¯" >}}
- ğŸ‘‰ To classify a new data point, every stump makes a prediction (+1 or -1). These are multiplied by their respective â€˜amount of say' and summed.
- ğŸ‘‰ If the total weighted ğŸ‹ï¸â€â™€ï¸ sum is positive, the final class is +1; otherwise -1.
- Note: Sensitive to outliers; Because AdaBoost aggressively increases weights ğŸ‹ï¸â€â™€ï¸ on misclassified points, it may â€˜over-focus' on noisy outliers, hurting performance.
\[H(x)=sign\sum_{m=1}^{M}\alpha_{m}â‹…h_{m}(x)\]
{{< /panel >}}

{{< panel color="green" title="Visual" >}}
{{< imgproc "images/machine_learning/supervised/decision_trees/adaboost/slide_08_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< video "https://youtu.be/KPcBlWoOOZo" >}}
<br><br>
```End of Section```