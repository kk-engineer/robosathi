---
title: GBDT Algorithm
description: GBDT Algorithm
date: 2026-02-14
weight: 2491
math: true
---

{{< playlist "https://www.youtube.com/playlist?list=PLnpa6KP2ZQxfYIFfsbRfK_M7gObPE_vwU" 
"Decision Tree | All Videos" >}}

<br>

{{< panel color="blue" title="Visual" >}}
{{< imgproc "images/machine_learning/supervised/decision_trees/gbdt_algorithm/slide_01_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="orange" title="Gradient Boosting Machine" >}}
- GBM treats the final model as weighted ğŸ‹ï¸â€â™€ï¸ sum of â€˜m' weak learners:
- : The initial base model (usually a constant).
- : The total number of boosting iterations (number of trees).
- (Leaf Weight): The optimized value for leaf in tree .
- (Nu): The Learning Rate or Shrinkage; prevent overfitting.
- : â€˜Indicator Function'; It is 1 if data point falls into leaf of the tree, and 0 otherwise.
- (Regions): Region of leaf in tree.
\[F_{M}(x)=F_{0}(x)_{âŸ}_{Initial Guess}+Î½\sum_{m=1}^{M}\sum_{j=1}^{J_{m}}\gamma_{jm}I(xâˆˆR_{jm})_{âŸ}_{Weak Learnerh_{m}(x)}\]
{{< /panel >}}

{{< panel color="green" title="Algorithm" >}}
- Step 1: Initialization.
- Step 2: Iterative loop ğŸ” : for i=1 to m.
- 2.1: Calculate pseudo residuals â€˜'.
- 2.2: Fit a regression tree ğŸŒ²â€˜'.
- 2.3:Compute leaf ğŸƒweights ğŸ‹ï¸â€â™€ï¸ â€˜'.
- 2.4:Update the model.
{{< /panel >}}

{{< panel color="red" title="Visual" >}}
{{< imgproc "images/machine_learning/supervised/decision_trees/gbdt_algorithm/slide_04_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="navy" title="Step 1: Initialization" >}}
- Start with a function that minimizes our loss function;
- for MSE its mean.
- MSE Loss =
\[F_{0}(x)=argmin_{\gamma}\sum_{i=1}^{n}L(y_{i},\gamma)\]
{{< /panel >}}

{{< panel color="blue" title="Step 2.1: Calculate pseudo residuals â€˜'" >}}
- Find the â€˜gradient' of error; Tells us the direction and magnitude needed to reduce the loss.
\[r_{im}=-\frac{âˆ‚L(y_{i},F(x_{i}))}{âˆ‚F(x_{i})}_{F(x)=F_{m-1}(x)}\]
{{< /panel >}}

{{< panel color="orange" title="Step 2.2: Fit regression tree â€˜'" >}}
- Train a tree to predict the residuals â€˜';
- Tree ğŸŒ² partitions the data into leaves ğŸƒ (regions )
{{< /panel >}}

{{< panel color="green" title="Step 2.3: Compute leaf weights â€˜'" >}}
- Within each leaf ğŸƒ, we calculate the optimal value â€˜' that minimizes the loss for the samples in that leaf ğŸƒ.
- â¡ï¸ The optimal leaf ğŸƒvalue is the â€˜Mean'(MSE) of the residuals ;
\[\gamma_{jm}=argmin_{\gamma}\sum_{x_{i}âˆˆR_{jm}}L(y_{i},F_{m-1}(x_{i})+\gamma)\]
{{< /panel >}}

{{< panel color="red" title="Step 2.4: Update the model." >}}
- Add the new â€˜correction' to the existing model, scaled by the learning rate.
- : â€˜Indicator Function'; It is 1 if data point falls into leaf of the tree, and 0 otherwise
\[F_{m}(x)=F_{m-1}(x)+Î½â‹…\sum_{j=1}^{J_{m}}\gamma_{jm}I(xâˆˆR_{jm})_{âŸ}_{h_{m}(x)}\]
{{< /panel >}}

{{< video "https://youtu.be/yf8a871iqt8" >}}
<br><br>
```End of Section```