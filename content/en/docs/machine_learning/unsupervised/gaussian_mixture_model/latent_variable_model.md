---
title: Latent Variable Model
description: Latent Variable Model
date: 2026-02-14
weight: 342
math: true
---

{{< playlist "https://www.youtube.com/playlist?list=PLnpa6KP2ZQxcF-fY0gn6FSxSgKT84rvpV" 
"Gaussian Mixture Model (GMM) | All Videos" >}}

<br>

{{< panel color="blue" title="Visual" >}}
{{< imgproc "images/machine_learning/unsupervised/gaussian_mixture_model/latent_variable_model/slide_01_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="orange" title="Gaussian Mixture Model (GMM)" >}}
- Probabilistic model that assumes data is generated from a mixture of several Gaussian (normal) distributions with unknown parameters.
- GMM represents the probability density function of the data as a weighted sum of â€˜K' component Gaussian densities.
{{< /panel >}}

{{< panel color="green" title="Gaussian Mixture PDF" >}}
- Overall density for a data point â€˜':
- : number of component Gaussians.
- : mixing coefficient (weight) of the component, such that, and .
- : probability density function of the Gaussian component with mean and covariance matrix .
- : complete set of parameters to be estimated.
\[p(x_{i}|\mu,\Sigma)=\sum_{k=1}^{K}\pi_{k}N(x_{i}|\mu_{k},\Sigma_{k})\]
{{< /panel >}}

{{< panel color="red" title="Visual" >}}
{{< imgproc "images/machine_learning/unsupervised/gaussian_mixture_model/latent_variable_model/slide_04_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="navy" title="Visual" >}}
{{< imgproc "images/machine_learning/unsupervised/gaussian_mixture_model/latent_variable_model/slide_05_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="blue" title="GMM Optimization (Why MLE Fails?)" >}}
- Goal ðŸŽ¯ of a GMM optimization is to find the set of parameters that maximize the likelihood of observing the given data.
- ðŸ¦€ cannot be simplified.
- ðŸ¦‰So, is there any other way ?
\[L(Î˜|X)=\sum_{i=1}^{N}logP(x_{i}|Î˜)=\sum_{i=1}^{N}log\sum_{k=1}^{K}\pi_{k}N(x_{i}|\mu_{k},\Sigma_{k})\]
{{< /panel >}}

{{< panel color="orange" title="Intuition ðŸ’¡(Latent Variable)" >}}
- Imagine we are measuring the heights of people in a college.
- We see a distribution with two peaks (bimodal).
- We suspect there are two underlying groups:
- Group A (Men) and Group B (Women).
- Observed Variable (X): Actual height measurements.
- Latent Variable (Z): The â€˜label' (Man or Woman) for each person.
- Note: We did not record this, so it is â€˜hidden' or â€˜latent'.
{{< /panel >}}

{{< panel color="green" title="Visual" >}}
{{< imgproc "images/machine_learning/unsupervised/gaussian_mixture_model/latent_variable_model/slide_08_01.png" Resize "1400x" >}}{{< /imgproc >}}
{{< /panel >}}

{{< panel color="red" title="Latent Variable Model" >}}
- A Latent Variable Model assumes that the observed data â€˜X' is generated by first picking a latent state â€˜z' and then drawing a sample from the distribution associated with that state.
{{< /panel >}}

{{< panel color="navy" title="GMM as Latent Variable Model" >}}
- GMM is a latent variable model, meaning each data point is assumed to have an associated unobserved (latent) variable indicating which component generated it.
- Note: We observe the data point but we do not observe which cluster it belongs to ().
{{< /panel >}}

{{< panel color="blue" title="Latent Variable Purpose" >}}
- If we knew the value of (component indicator) for every point, estimating the parameters of each Gaussian component would be straightforward.
- Note: The challenge lies in estimating both the parameters of the Gaussians and the values of the latent variables simultaneously.
{{< /panel >}}

{{< panel color="orange" title="Cluster Indicator (z) & Log Likelihood (sum)" >}}
- With â€˜z' unknown: maximize ðŸ¦€ cannot be simplified.
- With â€˜z' known: The log-likelihood of the â€˜complete data' simplifies into a sum of logarithms: ðŸ¦‰Every point is assigned to exactly one cluster, so the sum disappears because there is only one cluster responsible for that point.
- Note: This allows the logarithm to act directly on the exponential term of the Gaussian, leading to simple linear equations.
{{< /panel >}}

{{< panel color="green" title="Hard Assignment Simplifies Estimation" >}}
- When â€˜z' is known, every data point is â€˜labeled' with its parent component. To estimate the parameters (mean and covariance ) for a specific component â€˜k' :
- Gather all data points , where = k.
- Calculate the standard Maximum Likelihood Estimate.(MLE) for that single Gaussian using only those points.
{{< /panel >}}

{{< panel color="red" title="Closed-Form Solution" >}}
- Knowing â€˜z' provides exact counts and component assignments, leading to direct formulae for the parameters:
- Mean (): Arithmetic average of all points assigned to component â€˜k'.
- Covariance (): Sample covariance of all points assigned to component â€˜k'.
- Mixing Weight (): Fraction of total points assigned to component â€˜k'.
{{< /panel >}}

{{< video "https://youtu.be/PPeLhm38Bw8" >}}
<br><br>

<!-- nav-panel:start -->
<div style="display:flex;justify-content:space-between;align-items:center;width:100%;gap:16px;">
<span><a href="{{< ref "/docs/machine_learning/unsupervised/gaussian_mixture_model/introduction_gaussian_mixture_models" >}}">Previous: Introduction Gaussian Mixture Models</a></span>
<span style="margin-left:auto;"><a href="{{< ref "/docs/machine_learning/unsupervised/gaussian_mixture_model/expectation_maximization" >}}">Next: Expectation Maximization</a></span>
</div>
<!-- nav-panel:end -->

```End of Section```